---
sidebar_position: 1
---

# Vision-Language-Action ماڈلز

## ماڈیول کا جائزہ

یہ ماڈیول Vision-Language-Action (VLA) ماڈلز کا جامع تعارف فراہم کرتا ہے، جو جدید ترین ایمبوڈیڈ AI کی نمائندگی کرتے ہیں۔

## VLA ماڈلز کیا ہیں؟

Vision-Language-Action ماڈلز تین اہم اجزاء کو یکجا کرتے ہیں:

### Vision (بصارت)

- کیمرا تصاویر کا تجزیہ
- آبجیکٹ ڈیٹیکشن اور ریکگنیشن
- 3D منظر کی سمجھ

### Language (زبان)

- قدرتی زبان کی سمجھ
- انسانی ہدایات کی تعبیر
- سیاق و سباق کی بنیاد پر فیصلے

### Action (عمل)

- روبوٹ کنٹرول سگنلز
- ہاتھ اور بازو کی حرکت
- نیویگیشن کمانڈز

## اہم VLA ماڈلز

### RT-2 (Google)

Google کا روبوٹکس ٹرانسفارمر جو بصارت اور زبان کو عمل سے جوڑتا ہے۔

### OpenVLA

Stanford کا اوپن سورس VLA ماڈل جو:
- متعدد روبوٹ پلیٹ فارمز پر کام کرتا ہے
- فائن ٹیوننگ کی اجازت دیتا ہے
- تحقیق کے لیے دستیاب ہے

### π0 (Pi-Zero)

Physical Intelligence کا جنرلسٹ روبوٹ پالیسی ماڈل:
- متعدد ٹاسکس پر ٹرین کیا گیا
- ریئل ٹائم انفرنس
- مختلف روبوٹس پر منتقلی

## سیکھنے کے مقاصد

اس ماڈیول کے اختتام پر، آپ قابل ہو جائیں گے:
- VLA ماڈلز کے فن تعمیر کو سمجھنا
- پری ٹرینڈ ماڈلز کو لوڈ اور استعمال کرنا
- اپنے ڈیٹا پر فائن ٹیوننگ
- روبوٹ پر ڈپلائمنٹ

## استعمال کے معاملات

### ہیومنائیڈ روبوٹکس

- پیچیدہ مینیپولیشن ٹاسکس
- انسانی ہدایات کی سمجھ
- نئے ماحول میں موافقت

### صنعتی ایپلیکیشنز

- اسمبلی لائن آٹومیشن
- کوالٹی کنٹرول
- میٹریل ہینڈلنگ

## شروع کرنا

### ضروری لائبریریز

```bash
pip install torch transformers
pip install openai-vla  # OpenVLA کے لیے
```

### بنیادی مثال

```python
from vla import VLAModel

# ماڈل لوڈ کریں
model = VLAModel.from_pretrained("openvla-7b")

# انفرنس
action = model.predict(
    image=camera_image,
    instruction="کپ اٹھائیں"
)
```

## خلاصہ

VLA ماڈلز روبوٹکس کے مستقبل کی نمائندگی کرتے ہیں، جہاں روبوٹس قدرتی زبان کو سمجھ کر بصری معلومات کی بنیاد پر ذہین عمل کر سکتے ہیں۔
